Struttura dell'Architettura Transformer

l modello Transformer è composto principalmente da due componenti chiave: l'encoder e il decoder. Questi due componenti sono spesso usati insieme in diverse applicazioni di NLP, come la traduzione automatica, la generazione di testo e molte altre.

\subsubsection{Encoder}
L'encoder trasforma l'input (ad esempio, una sequenza di parole) in un insieme di rappresentazioni chiamate "embeddings" che catturano informazioni semantiche e strutturali. L'encoder è composto da più strati identici, ciascuno dei quali ha due componenti principali:

Multi-Head Self-Attention Layer: Questo è il cuore del modello Transformer. In questa parte, l'input (che è una sequenza di vettori) viene diviso in tre versioni: Query, Key e Value. L'attenzione self-attention calcola l'importanza delle diverse parti dell'input rispetto a ogni altra parte. Ciò consente al modello di catturare relazioni di lungo raggio tra le parole. Vengono eseguiti diversi calcoli di attenzione in parallelo (multi-head) per catturare diverse aspetti delle relazioni tra le parole.

Feedforward Neural Network: Dopo l'operazione di attenzione, l'output viene passato attraverso un'operazione di feedforward in cui ogni vettore di input viene trasformato tramite uno strato completamente connesso.

L'output di ogni strato dell'encoder è l'input per lo strato successivo. La pila di questi strati aiuta a catturare informazioni sempre più astratte e complesse dalle sequenze di input.

\subsubsection{Decoder}
Il decoder è responsabile di generare l'output a partire dalla rappresentazione creata dall'encoder. Anche il decoder è composto da diversi strati, ma include anche alcune differenze rispetto all'encoder:

Masked Multi-Head Self-Attention Layer: Invece di avere un'attenzione self-attention standard, il decoder utilizza un'attenzione self-attention mascherata. Ciò significa che in ogni posizione, una parola può "guardare" solo le parole che la precedono. Questo impedisce al modello di imbrogliare guardando le parole future durante la generazione.

Multi-Head Encoder-Decoder Attention Layer: Questo strato consente al decoder di concentrarsi sulle parti rilevanti dell'output dell'encoder. Aiuta a catturare le informazioni necessarie dall'input per generare l'output corretto.

Feedforward Neural Network: Simile all'encoder, il decoder ha anche strati di reti neurali feedforward per elaborare ulteriormente l'output.

In entrambi gli encoder e decoder, tra i vari strati, sono spesso utilizzati la normalizzazione batch e i collegamenti residui per facilitare l'allenamento e mitigare i problemi di vanishing gradient.

Quando si tratta di compiti specifici, come la traduzione automatica, il modello viene addestrato a generare la sequenza di output passo dopo passo, prendendo in considerazione l'output generato in passaggi precedenti. Durante l'allenamento, viene utilizzata una funzione di perdita che misura quanto bene l'output generato si avvicina all'output di riferimento desiderato.

Applicazioni nell'NLP
I Transformer sono stati applicati con successo a molteplici task di NLP, tra cui traduzione automatica, analisi del sentimento, generazione di testo, risposta alle domande e molto altro. Grazie alla capacità dell'architettura di catturare relazioni complesse, i modelli basati su Transformer hanno raggiunto o superato le prestazioni umane in molti di questi task.

Scalabilità e Parallelismo
Una caratteristica distintiva dei Transformer è la loro scalabilità orizzontale. A causa dell'indipendenza dei calcoli di attenzione tra le diverse parole di una sequenza, i Transformer possono eseguire calcoli paralleli su GPU e TPU, accelerando notevolmente l'addestramento e l'inferenza.

Limitazioni e Sviluppi Successivi
Nonostante i successi, i Transformer hanno alcune limitazioni, come l'incapacità di gestire sequenze molto lunghe senza perdita di contesto. Questo ha portato a ulteriori sviluppi, come gli architetti "Transformer XL" e "Longformer", che cercano di affrontare questa sfida.

In sintesi, i Transformer hanno cambiato il panorama dell'NLP, introducendo un'architettura altamente parallela e scalabile che ha dimostrato straordinarie capacità di catturare relazioni complesse nelle sequenze testuali. Il loro impatto si riflette in una serie di successivi modelli basati su Transformer, come BERT, GPT-2 e GPT-3.