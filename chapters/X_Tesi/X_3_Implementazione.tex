L'intero lavoro di tesi è stato svolto in Python\cite{python}, utilizzando un notebook Jupyter\cite{jupyter} come ambiente di esecuzione.

Le librerie selezionate per il progetto sono state di fondamentale importanza per il raggiungimento degli obiettivi prefissati:
\begin{itemize}
    \item \textbf{pypdf}: Questa libreria è stata impiegata per l'estrazione accurata del testo da documenti in formato PDF, consentendo così di lavorare con il contenuto testuale essenziale.
    \item \textbf{pandas}: Essenziale per la gestione efficiente del file XLSX contenente i metadati dei documenti. La flessibilità di Pandas ha facilitato la manipolazione e l'analisi dei dati correlati.
    \item \textbf{tiktoken}: L'implementazione del byte pair encoding (BPE) e della tokenizzazione tramite Tiktoken è stata cruciale per la preparazione dei testi e la loro successiva elaborazione nei modelli di OpenAI. 
    \item \textbf{openai}: La libreria ufficiale di OpenAI è stata utilizzata per interagire con i modelli e le API di OpenAI, consentendo un'interazione agevole e l'integrazione dei risultati nei processi di analisi.
    \item \textbf{langchain}: Questa libreria, che può essere considerata a tutti gli effetti un framework, è stata il fulcro centrale del lavoro di tesi. Grazie ad essa è stato possibile arricchire le capacità di un large language model con concetti avanzati, i quali saranno dettagliatamente esposti in seguito.
    \item \textbf{chromadb}: L'utilizzo della libreria ufficiale di ChromaDB ha consentito la conservazione e l'efficace interrogazione dei vettori di embeddings, rappresentando un passo cruciale per l'analisi e l'elaborazione dei dati ottenuti.
\end{itemize}

Il notebook è stato eseguito su piattaforma Google Colab, che oltre all'esecuzione permette di accedere allo storage di google drive, dove è stato conservato il dataset, in modo semplice e veloce come se fosse una directory della macchina in cui gira il notebook.

L'attenta scelta e l'integrazione di queste librerie hanno fornito gli strumenti necessari per l'elaborazione accurata e la valorizzazione dei dati, contribuendo significativamente al raggiungimento degli obiettivi di ricerca.

L'implementazione del progetto è stata suddivisa in due fasi principali: ingestion e inferenza.

\subsection{Ingestion} 
La fase di ingestion dei dati è descritta dal diagramma di flusso in figura 4.1:
\begin{figure}[H]
    \centering
    \includegraphics[height=0.5\pdfpageheight]{images/ingest.png}\label{fig:ingest}
    \caption[Ingestion]{Il diagramma di flusso che descrive la fase di ingest dei dati.}
\end{figure}

Come si può osservare, i dati sono stati conservati su Google Drive e il job di ingestion si occupa di leggere i 33 libri in formato pdf,
suddividerli in pagine assegnando loro le informazioni derivanti dai metadati, è presente inoltre una parte di enrichment di tali informazioni 
utilizzando il Large Language Model ``text-davinci-003'' di OpenAI per estrarre un riassunto di ogni pagina. Quest'ultima parte di enrichment è possibile espanderla ulteriormente
utilizzando l'LLM per estrarre ulteriori informazioni, come ad esempio le entità presenti in ogni pagina, ma per il momento è stata tralasciata.
La fase di enrichment inoltre serve ad arricchire i metadati di informazioni che, in seguito, il self-query retriever potrà utilizzare per interrogare il database.

Alimentato il database con le informazioni estratte, è possibile passare alla fase di inferenza.

\subsection[Inferenza]{Inferenza e interazione con l'utente}

La fase di inferenza e interazione con l'utente è descritta dal diagramma di flusso in figura 4.2:

\begin{figure}[H]
    \centering
    \includegraphics[height=0.3\pdfpageheight]{images/Inference.png}\label{fig:infer}
    \caption[Ingestion]{Il diagramma di flusso che descrive l'interazione dell'utente con il sistema di question answering.}
\end{figure}

Il job di inferenza si occupa di ricevere dall'utente una query in linguaggio naturale, la query passa in input al SelfQueryRetriever, 
che si occupa di interrogare il database e tramite una combinazione di selfquerying (riferimento ~\ref{fig:selfquery}) e similarity research restituisce una lista di documenti.
\begin{figure}
    \centering
    \includegraphics[width=0.7\pdfpagewidth]{images/selfquery.jpg}\label{fig:selfquery}
    \caption[Ingestion]{Come funziona un sistema di selfquerying.}
\end{figure}

Tali documenti vengono poi passati ad una catena di question answering, come contesto e, nella implementazione scelta, vengono presi in considerazione dal modello di OpenAI ``text-davinci-003'' che ciclicamente migliora la risposta precedente (figura \ref{fig:refine}).
In output vengono restituiti i documenti che il modello ha preso in considerazione e la risposta finale.

\begin{figure}
    \centering
    \includegraphics[width=0.7\pdfpagewidth]{images/refine.jpg}\label{fig:refine}
    \caption[Ingestion]{Come funziona una chain di tipo refine.}
\end{figure}

\subsubsection{La scelta di utilizzare le API di OpenAI}

Quando ci si approccia allo sviluppo di un sistema basato su LLM si hanno due principali scelte:
\begin{itemize}
    \item Utilizzare un modello installato in locale
    \item Utilizzare un modello tramite API di terze parti
\end{itemize}

La prima scelta è quella che garantisce maggiore flessibilità e controllo sul modello, ma richiede un hardware molto potente e un'ottima conoscenza di come funzionano i LLM.
La seconda scelta è quella che garantisce maggiore semplicità di utilizzo e un'ottima qualità dei risultati, ma richiede un budget per le API.

Entrambe le scelte, comunque, impongono l'ulteriore scelta di scegliere tra un modello commerciale e un modello opensource.

Nel caso si scelga un modello opensource uno dei metodi più utilizzati è quello di utilizzare le utilies che mette a disposizione il noto sito di repository per l'AI HuggingFace \url{https://huggingface.co/} che con la sua libreria "Transformers" permette sia di effettuare il download di un modello opensource che di utilizzarne uno su una macchina di HuggingFace tramite il suo HuggingFaceHub.
La scelta di un modello opensource pone delle questioni da tenere in considerazione come la non uniformità su come il modello si utilizzi, per esempio moltissime differenze si riscontrano quando si vuole effettuare un fine-tuning e ogni modello ha un formato specifico con cui deve essere creato il dataset o anche il fatto che magari un modello ha bisogno di una specifica versione di alcune dipendenze che magari altri modelli non supportano.

Utilizzando invece un modello commerciale, come quelli di OpenAI, si ha la certezza che il modello sia sempre aggiornato e che sia sempre disponibile, inoltre si ha la certezza che il modello sia sempre utilizzabile con le stesse modalità e che non ci siano problemi di dipendenze o di formati di dataset.
Nel lavoro di tesi, per esempio, è stato utilizzato il modello ``text-davinci-003'' di OpenAI ma a livello programmatico nulla sarebbe cambiato se io avessi utilizzato un modello minore come il ``ada'' o il ``babbage''.